{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23cf1c67",
   "metadata": {},
   "source": [
    "# RAG with OpenAI embeddings + FAISS\n",
    "Build a simple PDF question-answering pipeline using OpenAI embeddings and a local FAISS vector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cae0b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a package is missing, uncomment and run the line below.\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e025192",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "try:\n",
    "    import faiss  # from faiss-cpu\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"faiss is missing. Install with `pip install faiss-cpu` or `%pip install faiss-cpu`.\" ) from exc\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2df3658",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"Set OPENAI_API_KEY as an environment variable before continuing.\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "CHAT_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "CHUNK_MIN_WORDS = 150\n",
    "CHUNK_MAX_WORDS = 250\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a7781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunk_text(text: str, min_words: int = CHUNK_MIN_WORDS, max_words: int = CHUNK_MAX_WORDS) -> List[str]:\n",
    "    \"\"\"Chunk text by paragraph, keeping boundaries; merge or split to stay within word limits.\"\"\"\n",
    "    if max_words <= 0 or min_words <= 0 or min_words > max_words:\n",
    "        raise ValueError(\"Invalid min/max word configuration.\")\n",
    "\n",
    "    paragraphs: List[str] = []\n",
    "    buffer: List[str] = []\n",
    "    for line in text.splitlines():\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            buffer.append(line)\n",
    "        elif buffer:\n",
    "            paragraphs.append(\" \".join(buffer))\n",
    "            buffer = []\n",
    "    if buffer:\n",
    "        paragraphs.append(\" \".join(buffer))\n",
    "\n",
    "    if not paragraphs:\n",
    "        return []\n",
    "\n",
    "    def split_long_paragraph(words: List[str]) -> List[str]:\n",
    "        chunks: List[List[str]] = []\n",
    "        idx = 0\n",
    "        n = len(words)\n",
    "        while idx < n:\n",
    "            remaining = n - idx\n",
    "            if remaining > max_words:\n",
    "                take = max_words\n",
    "            elif remaining < min_words and chunks:\n",
    "                chunks[-1].extend(words[idx:])\n",
    "                break\n",
    "            else:\n",
    "                take = remaining\n",
    "            chunks.append(words[idx:idx + take])\n",
    "            idx += take\n",
    "        return [\" \".join(chunk) for chunk in chunks]\n",
    "\n",
    "    chunks: List[str] = []\n",
    "    current: List[str] = []\n",
    "    current_words = 0\n",
    "\n",
    "    def flush_current():\n",
    "        nonlocal current_words\n",
    "        if current:\n",
    "            chunks.append(\" \".join(current))\n",
    "            current = []\n",
    "            current_words = 0\n",
    "\n",
    "    for para in paragraphs:\n",
    "        words = para.split()\n",
    "        wcount = len(words)\n",
    "\n",
    "        if wcount > max_words:\n",
    "            flush_current()\n",
    "            chunks.extend(split_long_paragraph(words))\n",
    "            continue\n",
    "\n",
    "        if not current:\n",
    "            current = [para]\n",
    "            current_words = wcount\n",
    "            continue\n",
    "\n",
    "        if current_words + wcount <= max_words:\n",
    "            current.append(para)\n",
    "            current_words += wcount\n",
    "            continue\n",
    "\n",
    "        if current_words < min_words:\n",
    "            current.append(para)\n",
    "            current_words += wcount\n",
    "            flush_current()\n",
    "        else:\n",
    "            flush_current()\n",
    "            current = [para]\n",
    "            current_words = wcount\n",
    "\n",
    "    if current_words:\n",
    "        if current_words < min_words and chunks:\n",
    "            chunks[-1] = chunks[-1] + \" \" + \" \".join(current)\n",
    "        else:\n",
    "            flush_current()\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def load_pdf(pdf_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract page-level text from a PDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages = []\n",
    "    try:\n",
    "        for page in doc:\n",
    "            text = page.get_text(\"text\") or \"\"\n",
    "            pages.append({\"page\": page.number + 1, \"text\": text.strip()})\n",
    "    finally:\n",
    "        doc.close()\n",
    "    return pages\n",
    "\n",
    "\n",
    "def build_documents(pages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    docs: List[Dict[str, Any]] = []\n",
    "    for page in pages:\n",
    "        for idx, chunk in enumerate(chunk_text(page[\"text\"])):\n",
    "            docs.append({\n",
    "                \"id\": f\"p{page['page']}_c{idx}\",\n",
    "                \"page\": page[\"page\"],\n",
    "                \"text\": chunk,\n",
    "            })\n",
    "    return docs\n",
    "\n",
    "\n",
    "def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "    response = client.embeddings.create(model=EMBED_MODEL, input=texts)\n",
    "    vectors = np.array([item.embedding for item in response.data], dtype=\"float32\")\n",
    "    return vectors\n",
    "\n",
    "\n",
    "class FaissStore:\n",
    "    def __init__(self, dim: int):\n",
    "        self.index = faiss.IndexFlatL2(dim)\n",
    "        self.meta: List[Dict[str, Any]] = []\n",
    "\n",
    "    def add(self, embeddings: np.ndarray, metadatas: List[Dict[str, Any]]):\n",
    "        if embeddings.shape[0] != len(metadatas):\n",
    "            raise ValueError(\"Embeddings and metadata counts do not match.\")\n",
    "        self.index.add(embeddings)\n",
    "        self.meta.extend(metadatas)\n",
    "\n",
    "    def search(self, query_embedding: np.ndarray, k: int = 5) -> List[Dict[str, Any]]:\n",
    "        query_embedding = np.array([query_embedding], dtype=\"float32\")\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        results = []\n",
    "        for dist, idx in zip(distances[0], indices[0]):\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            results.append({\"score\": float(dist), **self.meta[idx]})\n",
    "        return results\n",
    "\n",
    "    def save(self, index_path: str = \"faiss.index\", meta_path: str = \"metadata.json\"):\n",
    "        faiss.write_index(self.index, index_path)\n",
    "        with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, index_path: str = \"faiss.index\", meta_path: str = \"metadata.json\") -> \"FaissStore\":\n",
    "        index = faiss.read_index(index_path)\n",
    "        with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            meta = json.load(f)\n",
    "        store = cls(index.d)\n",
    "        store.index = index\n",
    "        store.meta = meta\n",
    "        return store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a418e1f",
   "metadata": {},
   "source": [
    "## 1) Ingest a PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2429ae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pdf_path = \"./path/to/your.pdf\"  # update this to your PDF\n",
    "pages = load_pdf(pdf_path)\n",
    "print(f\"Loaded {len(pages)} pages from {pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8f4fe6",
   "metadata": {},
   "source": [
    "## 2) Chunk and embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6549c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = build_documents(pages)\n",
    "print(f\"Prepared {len(documents)} chunks\")\n",
    "\n",
    "embeddings = embed_texts([doc[\"text\"] for doc in documents])\n",
    "vector_dim = embeddings.shape[1]\n",
    "store = FaissStore(vector_dim)\n",
    "store.add(embeddings, documents)\n",
    "print(f\"FAISS index built with dimension {vector_dim}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657e2572",
   "metadata": {},
   "source": [
    "## 3) Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943c3528",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve(query: str, k: int = 5):\n",
    "    query_vec = embed_texts([query])[0]\n",
    "    results = store.search(query_vec, k=k)\n",
    "    for res in results:\n",
    "        preview = res['text'][:140].replace('\\n', ' ')\n",
    "        print(f\"Page {res['page']} (score={res['score']:.4f}): {preview}...\")\n",
    "    return results\n",
    "\n",
    "sample_results = retrieve(\"What is this document about?\", k=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd24f794",
   "metadata": {},
   "source": [
    "## 4) Generate answer with retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e913b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def answer(query: str, k: int = 5) -> str:\n",
    "    hits = retrieve(query, k=k)\n",
    "    context = \"\\n\\n\".join([f\"[p{hit['page']}] {hit['text']}\" for hit in hits])\n",
    "    prompt = (\n",
    "        \"You are a concise assistant. Use the provided context to answer the question. \"\n",
    "        \"Cite pages in brackets like [p2]. If unsure, say you are not sure.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Answer using only the provided context.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(answer(\"Give me a two sentence summary.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f920bf",
   "metadata": {},
   "source": [
    "## 5) Persist / reload index (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10c42a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save\n",
    "store.save(\"faiss.index\", \"metadata.json\")\n",
    "\n",
    "# Reload (example)\n",
    "# store = FaissStore.load(\"faiss.index\", \"metadata.json\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
