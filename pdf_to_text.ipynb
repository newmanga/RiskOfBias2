{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "80eca486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ea695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load OPENAI_API_KEY from environment or fallback to .env\n",
    "env_path = Path('.env')\n",
    "if env_path.exists():\n",
    "    for line in env_path.read_text().splitlines():\n",
    "        if line.strip().startswith('#') or '=' not in line:\n",
    "            continue\n",
    "        key, value = line.split('=', 1)\n",
    "        if key.strip() == 'OPENAI_API_KEY' and value.strip():\n",
    "            os.environ.setdefault('OPENAI_API_KEY', value.strip())\n",
    "            break\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError('Set OPENAI_API_KEY in your environment or .env')\n",
    "\n",
    "# create an OpenAI client using the API key\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1404f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pymupdf4llm\n",
    "# md_text = pymupdf4llm.to_markdown(\"6832_SercePehlevan_2020.pdf\")\n",
    "\n",
    "# import pathlib\n",
    "# pathlib.Path(\"output.md\").write_bytes(md_text.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c61c563",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = client.files.create(\n",
    "    file=open(\"studies/6832_SercePehlevan_2020.pdf\", \"rb\"),\n",
    "    purpose=\"user_data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "beca6537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Assessment(BaseModel):\n",
    "    answer: str\n",
    "    justification: str\n",
    "    citations: list[str]\n",
    "\n",
    "\n",
    "from typing_extensions import TypedDict, Literal\n",
    "from typing import List\n",
    "\n",
    "class RobAnswer(TypedDict):\n",
    "    answer: Literal[\"Y\", \"PY\", \"NI\", \"PN\", \"N\"]\n",
    "    justification: str\n",
    "    citations: List[str]\n",
    "\n",
    "def generate_response_with_chatgpt(prompt, file_id):\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        # model=\"gpt-4o-2024-08-06\",\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    { \"type\": \"input_text\", \"text\": prompt },\n",
    "                    {\n",
    "                        \"type\": \"input_file\",\n",
    "                        \"file_id\": file_id,\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        text={\n",
    "            \"format\": {\n",
    "                \"type\": \"json_schema\",\n",
    "                \"name\": \"response_details\",\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"answer\": {\"type\": \"string\"},\n",
    "                        \"justification\": {\"type\": \"string\"},\n",
    "                        \"citations\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                    },\n",
    "                    \"required\": [\"answer\", \"justification\", \"citations\"],\n",
    "                    \"additionalProperties\": False,\n",
    "                },\n",
    "                \"strict\": True,\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "40cc6dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"prompts/domain_1_randomization/question_1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     content = f.read()\n",
    "\n",
    "# answer = generate_response_with_chatgpt(content, file.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02e964d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt question files by domain (variants split via folder name):\n",
      "domain_1_randomization\n",
      " - 1.1: prompts/domain_1_randomization/question_1.txt\n",
      " - 1.2: prompts/domain_1_randomization/question_2.txt\n",
      " - 1.3: prompts/domain_1_randomization/question_3.txt\n",
      "domain_2_adhering\n",
      " - 2.1: prompts/domain_2_adhering/question_1.txt\n",
      " - 2.2: prompts/domain_2_adhering/question_2.txt\n",
      " - 2.3: prompts/domain_2_adhering/question_3.txt\n",
      " - 2.4: prompts/domain_2_adhering/question_4.txt\n",
      " - 2.5: prompts/domain_2_adhering/question_5.txt\n",
      " - 2.6: prompts/domain_2_adhering/question_6.txt\n",
      "domain_2_assigment\n",
      " - 2.1: prompts/domain_2_assigment/question_1.txt\n",
      " - 2.2: prompts/domain_2_assigment/question_2.txt\n",
      " - 2.3: prompts/domain_2_assigment/question_3.txt\n",
      " - 2.4: prompts/domain_2_assigment/question_4.txt\n",
      " - 2.5: prompts/domain_2_assigment/question_5.txt\n",
      " - 2.6: prompts/domain_2_assigment/question_6.txt\n",
      " - 2.7: prompts/domain_2_assigment/question_7.txt\n",
      "domain_3_missing_data\n",
      " - 3.1: prompts/domain_3_missing_data/question_1.txt\n",
      " - 3.2: prompts/domain_3_missing_data/question_2.txt\n",
      " - 3.3: prompts/domain_3_missing_data/question_3.txt\n",
      " - 3.4: prompts/domain_3_missing_data/question_4.txt\n",
      "domain_4_measurement\n",
      " - 4.1: prompts/domain_4_measurement/question_1.txt\n",
      " - 4.2: prompts/domain_4_measurement/question_2.txt\n",
      " - 4.3: prompts/domain_4_measurement/question_3.txt\n",
      " - 4.4: prompts/domain_4_measurement/question_4.txt\n",
      " - 4.5: prompts/domain_4_measurement/question_5.txt\n",
      "domain_5_reporting\n",
      " - 5.1: prompts/domain_5_reporting/question_1.txt\n",
      " - 5.2: prompts/domain_5_reporting/question_2.txt\n",
      " - 5.3: prompts/domain_5_reporting/question_3.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# Map domain (variants via folder name) -> question code -> prompt question file path\n",
    "prompt_question_files = defaultdict(dict)\n",
    "pattern = re.compile(r\"^domain_?(\\d+)(?:_(.+))?$\")\n",
    "for prompt_path in Path('prompts').rglob('question_*.txt'):\n",
    "    match = pattern.match(prompt_path.parent.name)\n",
    "    if not match:\n",
    "        continue\n",
    "    domain_id, variant = match.groups()\n",
    "\n",
    "    stem_parts = prompt_path.stem.split('_')\n",
    "    qnum = stem_parts[1] if len(stem_parts) > 1 else 'unknown'\n",
    "    question_code = f\"{domain_id}.{qnum}\"\n",
    "\n",
    "    domain_key = f\"domain_{domain_id}\" if not variant else f\"domain_{domain_id}_{variant}\"\n",
    "    prompt_question_files[domain_key][question_code] = str(prompt_path)\n",
    "\n",
    "# Sort questions for stable output\n",
    "prompt_question_files = {\n",
    "    domain: {code: path for code, path in sorted(questions.items())}\n",
    "    for domain, questions in sorted(prompt_question_files.items())\n",
    "}\n",
    "\n",
    "print('Prompt question files by domain (variants split via folder name):')\n",
    "for domain, questions in prompt_question_files.items():\n",
    "    print(domain)\n",
    "    for code, f in questions.items():\n",
    "        print(f\" - {code}: {f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ff5075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read PDF text, load a domain module, walk signalling questions, and store responses to Excel\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE\n",
    "from rob2.common import Response\n",
    "from rob2.domains import get_domain_specs\n",
    "\n",
    "def clean_excel(val):\n",
    "    if isinstance(val, str):\n",
    "        return ILLEGAL_CHARACTERS_RE.sub(\"\", val)\n",
    "    if isinstance(val, list):\n",
    "        return [clean_excel(v) for v in val]\n",
    "    if isinstance(val, dict):\n",
    "        return {k: clean_excel(v) for k, v in val.items()}\n",
    "    return val\n",
    "\n",
    "# Load domain specs once\n",
    "DOMAIN_SPECS = get_domain_specs()\n",
    "\n",
    "pdf_paths = sorted(Path('studies').glob('*.pdf'))\n",
    "\n",
    "if not pdf_paths:\n",
    "    print('No PDF files found in studies/.')\n",
    "else:\n",
    "    for pdf_path in pdf_paths:\n",
    "        print(f\"Processing {pdf_path.name}\")\n",
    "        file = client.files.create(\n",
    "            file=open(pdf_path, \"rb\"),\n",
    "            purpose=\"user_data\",\n",
    "        )\n",
    "\n",
    "        rows = []\n",
    "\n",
    "        for domain_key in prompt_question_files:\n",
    "            state = {}\n",
    "            domain_prompts = prompt_question_files[domain_key]\n",
    "            spec = DOMAIN_SPECS.get(domain_key)\n",
    "            if spec is None:\n",
    "                print(f\"Skipping {domain_key}: no spec registered.\")\n",
    "                continue\n",
    "\n",
    "            get_next_question = spec.get_next_question\n",
    "            questions = spec.questions\n",
    "\n",
    "            question_code = get_next_question(state)\n",
    "            while question_code:\n",
    "                prompt_path = Path(domain_prompts.get(question_code, ''))\n",
    "                prompt_text = prompt_path.read_text(encoding='utf-8') if prompt_path.exists() else ''\n",
    "                question_text = questions.get(question_code, '')\n",
    "\n",
    "                print(f\"{domain_key} -> {question_code}\")\n",
    "                if not prompt_text:\n",
    "                    print('Prompt not found for this question code.')\n",
    "\n",
    "                response_raw = generate_response_with_chatgpt(prompt_text, file.id)\n",
    "                response = json.loads(response_raw)\n",
    "\n",
    "                answer = response.get(\"answer\", \"\")\n",
    "                justification = response.get(\"justification\", \"\")\n",
    "                citations = response.get(\"citations\", [])\n",
    "\n",
    "                print(answer)\n",
    "                print(justification)\n",
    "                print(citations)\n",
    "\n",
    "                state[question_code] = Response(answer ) # store string\n",
    "                rows.append({\n",
    "                    \"file_name\": pdf_path.name,\n",
    "                    \"domain\": domain_key,\n",
    "                    \"question_code\": question_code,\n",
    "                    \"question_text\": question_text,\n",
    "                    \"prompt_path\": str(prompt_path),\n",
    "                    \"answer\": answer,\n",
    "                    \"justification\": clean_excel(justification),\n",
    "                    \"citations\": clean_excel(\"; \".join(citations)) if isinstance(citations, list) else str(citations),\n",
    "                })\n",
    "\n",
    "                time.sleep(15)\n",
    "\n",
    "                question_code = get_next_question(state)\n",
    "\n",
    "        output_dir = Path('outputs')\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        output_file = output_dir / f\"{pdf_path.stem}_responses.xlsx\"\n",
    "        pd.DataFrame(rows).to_excel(output_file, index=False)\n",
    "        print(f\"Saved {len(rows)} rows to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fde305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain_4_measurement -> 4.1\n",
      "N\n",
      "The outcome measurement methods were appropriate for the outcomes being assessed. The primary outcome—infant crying/fussing time—was measured using the validated Barr diary, a recognized tool for this purpose, with caregiver instructions standardized by nurses. Stool consistency and frequency were also measured using established scales, and parental quality of life was assessed with a validated questionnaire (PedsQLTM). Standardized, validated, and accepted approaches were applied consistently across groups at pre-specified timepoints.\n",
      "['\"The validated Barr diary (17) was used to record the infant colicky full force crying/fussing time (mins/day), number of episodes of colicky full force crying/fussing/day, stool consistency, and stool frequency. Stool consistency on diapers was scored as 0 for watery stool, 1 for loose stool, 2 for formed stool, and 3 for hard stool as per Amsterdam consistency subscale (18). PedsQLTM, a 15-item validated questionnaire, was used to assess family functioning (19)...\"', '\"The nurses explained to the children’s caregivers how to properly record all the information. There were no changes to trial outcome measures after the trial commenced.\"', '\"Caregivers, who received the standardized operation training from nurses at the time of recruitment, administered five drops of the study product orally to each infant, daily for 21 days.\"']\n",
      "domain_4_measurement -> 4.2\n",
      "N\n",
      "The outcome measurement procedures were equivalent across intervention groups. The study was double-blinded, with identical placebo and probiotic administration protocols, and outcome assessment was based on validated tools used at the same time points for both groups. There is no evidence or statement suggesting any differences in measurement methods, timing, follow-up, or assessor behavior between groups.\n",
      "[\"'The study was double-blinded whereby treatment allocation was concealed from all study investigators and participants. ... Both the probiotic and the placebo were manufactured ... Products are routinely shipped ... in containers controlled for temperature with data loggers. ... The products were used before reaching the expiry date.'\", \"'Caregivers, who received the standardized operation training from nurses at the time of recruitment, administered five drops of the study product orally to each infant, daily for 21 days.'\", \"'The primary outcome (daily crying or fussing time) and secondary outcome (number of crying/fussing episodes, fecal consistency, and parental quality of life) assessments were adapted from Sung et al. with minor modifications (16) and were measured in all study visits.'\", \"'There were no significant differences in gender, allergy history, age, birth weight, passive smoking exposure, living environment, parents’ educational levels ... and feeding method during the intervention between the two groups (p > 0.05).'\", \"'No treatment-related side effects occurred during the study period.'\"]\n",
      "Saved 2 rows to outputs/990_Chen_2021_domain_4_measurement_responses.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Run a single domain for one PDF (requires setup cells above for client/prompts)\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from rob2.common import Response\n",
    "\n",
    "# Set the PDF to evaluate and the target domain key\n",
    "single_pdf = Path(\"studies/990_Chen_2021.pdf\")\n",
    "single_domain = \"domain_4_measurement\"\n",
    "\n",
    "if \"prompt_question_files\" not in globals() or \"DOMAIN_SPECS\" not in globals():\n",
    "    raise RuntimeError(\"Run the prompt/domain setup cells first (cells 6 and 7).\")\n",
    "\n",
    "if not single_pdf.exists():\n",
    "    raise FileNotFoundError(f\"PDF not found: {single_pdf}\")\n",
    "\n",
    "if single_domain not in prompt_question_files:\n",
    "    raise ValueError(f\"Unknown domain key: {single_domain}\")\n",
    "\n",
    "spec = DOMAIN_SPECS.get(single_domain)\n",
    "if spec is None:\n",
    "    raise ValueError(f\"No spec registered for domain: {single_domain}\")\n",
    "\n",
    "domain_prompts = prompt_question_files[single_domain]\n",
    "state = {}\n",
    "rows = []\n",
    "\n",
    "file = client.files.create(\n",
    "    file=open(single_pdf, \"rb\"),\n",
    "    purpose=\"user_data\",\n",
    ")\n",
    "\n",
    "question_code = spec.get_next_question(state)\n",
    "while question_code:\n",
    "    prompt_path = Path(domain_prompts.get(question_code, \"\"))\n",
    "    prompt_text = prompt_path.read_text(encoding=\"utf-8\") if prompt_path.exists() else \"\"\n",
    "    question_text = spec.questions.get(question_code, \"\")\n",
    "\n",
    "    print(f\"{single_domain} -> {question_code}\")\n",
    "    if not prompt_text:\n",
    "        print(\"Prompt not found for this question code.\")\n",
    "\n",
    "    response_raw = generate_response_with_chatgpt(prompt_text, file.id)\n",
    "    response = json.loads(response_raw)\n",
    "\n",
    "    answer = response.get(\"answer\", \"\")\n",
    "    justification = response.get(\"justification\", \"\")\n",
    "    citations = response.get(\"citations\", [])\n",
    "\n",
    "    print(answer)\n",
    "    print(justification)\n",
    "    print(citations)\n",
    "\n",
    "    state[question_code] = Response(answer)\n",
    "    rows.append({\n",
    "        \"file_name\": single_pdf.name,\n",
    "        \"domain\": single_domain,\n",
    "        \"question_code\": question_code,\n",
    "        \"question_text\": question_text,\n",
    "        \"prompt_path\": str(prompt_path),\n",
    "        \"answer\": answer,\n",
    "        \"justification\": clean_excel(justification),\n",
    "        \"citations\": clean_excel(\"; \".join(citations)) if isinstance(citations, list) else str(citations),\n",
    "    })\n",
    "\n",
    "    time.sleep(15)\n",
    "    question_code = spec.get_next_question(state)\n",
    "\n",
    "output_dir = Path(\"outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "output_file = output_dir / f\"{single_pdf.stem}_{single_domain}_responses.xlsx\"\n",
    "pd.DataFrame(rows).to_excel(output_file, index=False)\n",
    "print(f\"Saved {len(rows)} rows to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df06655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
